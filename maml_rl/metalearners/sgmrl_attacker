import torch

from torch.nn.utils.convert_parameters import parameters_to_vector
from torch.distributions.kl import kl_divergence

from maml_rl.samplers import MultiTaskSampler
from maml_rl.metalearners.base import GradientBasedMetaLearner
from maml_rl.utils.torch_utils import (weighted_mean, detach_distribution,
                                       to_numpy, vector_to_parameters)
from maml_rl.utils.optimization import conjugate_gradient
from maml_rl.utils.reinforcement_learning import reinforce_loss
from maml_rl.utils.global_tensor_val import get_value

class SGMRLATTACKER(GradientBasedMetaLearner):
    
    def __init__(self,
                 policy,
                 fast_lr=0.5,
                 first_order=False,
                 device='cuda'):
        super(MAMLTRPO, self).__init__(policy, device=device)
        self.fast_lr = fast_lr
        self.first_order = first_order

    async def adapt(self, train_futures, first_order=None, Adv=1):
        if first_order is None:
            first_order = self.first_order
        # Loop over the number of steps of adaptation
        params = None
        for futures in train_futures:
            ##print(get_value('Adv'))
            inner_loss = reinforce_loss(self.policy,
                                        await futures,
                                        params=params)
            inner_loss= Adv * inner_loss
            ##print(Adv)
            params = self.policy.update_params(inner_loss,
                                               params=params,
                                               step_size=self.fast_lr,
                                               first_order=first_order)
        return params